---
title: "p8105_hw6_jl5549"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(modelr )
```

# Problem1
```{r}
birthweight = read_csv(file = './data/birthweight.csv') %>% 
  janitor::clean_names() %>% 
  mutate(babysex = as.factor(babysex),
         momage = as.factor(momage)) %>% 
  drop_na()
head(birthweight)
```

```{r}
mymodel =step(lm(bwt ~ ., data = birthweight), direction = "backward", k=log(nrow(birthweight)))
summary(mymodel)
#a plot of model residuals against fitted values
birthweight %>% 
  add_predictions(mymodel) %>% 
  add_residuals(mymodel) %>% 
  ggplot(aes(x = pred, y = resid))+
  geom_point(col = 'brown') +
  geom_line(stat = "smooth", method = "lm")
broom::glance(mymodel)

```

I used "Backward Elimination" method to build my model to remove the variable with  lowest significance every time.


```{r}
cv_df =
  birthweight %>% 
  crossv_mc(.,n = 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) 
#calculate there models' rmse
cv_df= 
  cv_df%>% 
  mutate(
    model1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks +  mrace + ppwt + smoken, data = .)),
    model2 = map(train, ~lm(bwt~blength+gaweeks, data = .)),
    model3 = map(train, ~lm(bwt~bhead*blength*babysex, data= .))
    ) %>% 
  mutate(rmse_mymodel = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))
#compare three models 
cv_df %>% 
  select(starts_with('rmse')) %>%
  pivot_longer(everything(),
    names_to = 'model',
               values_to = 'rmse',
               names_prefix = 'rmse_')%>% 
  mutate(model= factor(model, levels = c('mymodel', 'model2', 'model3'))) %>% 
  ggplot(aes(x= model , y = rmse))+
  geom_violin()
```

According to the violin plot above we can see my model has lowest rmse which means it does better in prediction than other two models. And model2 has highest rmse. It has lowest predictive accuracy.

## Problem 2
```{r}
#import the data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
#boostrap datasets and build model for each
weather_models = 
weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(models = map(strap, ~lm(tmax~tmin, data = .x))) 
#tidy the models and extract statistics
estimates = 
  weather_models%>% 
  mutate(results = map(models, broom::tidy)) %>% 
  select(-models, -strap) %>% 
  unnest(results) %>% 
  pivot_wider(values_from = 'estimate',
              names_from = 'term') %>%
  janitor::clean_names() 
#extract beta0 and beta1
intercept = 
  estimates %>% 
   select(intercept, id) %>% 
   drop_na()
tmin = 
  estimates %>% 
   select(tmin, id) %>% 
   drop_na()
#calculate log(β̂ 0∗β̂ 1)
beta = inner_join(intercept, tmin, by = "id") %>% 
  mutate(log_beta01 = log(intercept*tmin))
#distribution of log(β̂ 0∗β̂ 1)
beta %>% 
  ggplot(aes(x =log_beta01))+
  geom_density()+
  labs(x = 'log(estimated beta0 *estimated beta1)')
```


```{r}
#2.5% and 97.5% quantiles
q1_log_beta01= quantile(beta$log_beta01, 0.025)
q2_log_beta01 = quantile(beta$log_beta01, 0.975)
# construct CIs
se_log_beta01 = sd(beta$log_beta01)/sqrt(length(beta$log_beta01))
mean_log_beta01 = mean(beta$log_beta01)
CI_log_beta01 = c(mean_log_beta01 - qnorm(0.975)*se_log_beta01,
                  mean_log_beta01 + qnorm(0.975)*se_log_beta01)
CI_log_beta01
```

The shape of log(estimated beta0 *estimated beta1) is close to normal. The mean is `r {mean_log_beta01}`.
The CI  of log(beta0 * beta1)  is (`r {CI_log_beta01}`)

```{r}
#find r^ 2
r2 =   
  weather_models%>% 
  mutate(results = map(models, broom::glance)) %>% 
  select(-models, -strap) %>% 
  unnest(results) %>% 
  janitor::clean_names()
r2 %>% 
  ggplot(aes(x = r_squared))+
  geom_density()
```


```{r}
#2.5% and 97.5% quantiles
q1_r2= quantile(r2$r_squared, 0.025)
q2_r2= quantile(r2$r_squared, 0.975)
# construct CIs
se_rsquared = sd(r2$r_squared)/sqrt(length(r2$r_squared))
mean_rsquared = mean(r2$r_squared)
CI_rsquared = c(mean_rsquared - qnorm(0.975)*se_rsquared,
                  mean_rsquared + qnorm(0.975)*se_rsquared)
CI_rsquared

```

The shape of r^2 is close to normal. The mean is`r {mean_rsquared}`.
The CI of r^2 is (`r {CI_rsquared}`)
